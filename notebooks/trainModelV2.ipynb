{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('../data/processed/training_set.json', encoding='utf8')\n",
    "data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted from : \n",
      " [['irrelevant']\n",
      " ['irrelevant']\n",
      " ['purchase']\n",
      " ['find-hotel']\n",
      " ['irrelevant']\n",
      " ['irrelevant']\n",
      " ['irrelevant']\n",
      " ['irrelevant']\n",
      " ['purchase']\n",
      " ['purchase']]\n",
      "\n",
      "to one-hot encoded format : \n",
      " [[0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y_training_raw = np.array([datapoint[\"intent\"] for datapoint in data])\n",
    "x_training = np.array([datapoint[\"sentence\"] for datapoint in data])\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_training = encoder.fit_transform(y_training_raw.reshape(-1,1))\n",
    "print(\"converted from : \\n\", encoder.inverse_transform(y_training[:10,:]))\n",
    "print(\"\\nto one-hot encoded format : \\n\", y_training[:10,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['find-around-me', 'find-flight', 'find-hotel', 'find-restaurant',\n",
       "        'find-train', 'irrelevant', 'provide-showtimes', 'purchase'],\n",
       "       dtype='<U17')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can I get a list ?\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1.]  :  Le meilleur cabriolet hybrid moins de 5m10 minimum 400 litres de coffre ?\n",
      "6035 6035\n"
     ]
    }
   ],
   "source": [
    "print(y_training[2], \" : \",x_training[2])\n",
    "print(len(y_training),len(x_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner', 'textcat']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "# Adding the built-in textcat component to the pipeline.\n",
    "textcat=nlp.create_pipe( \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"})\n",
    "nlp.add_pipe(textcat, last=True)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find-around-me\n",
      "find-flight\n",
      "find-hotel\n",
      "find-restaurant\n",
      "find-train\n",
      "irrelevant\n",
      "provide-showtimes\n",
      "purchase\n"
     ]
    }
   ],
   "source": [
    "# Add categories to the tagger\n",
    "for cat in encoder.categories_[0]:\n",
    "    print(cat)\n",
    "    textcat.add_label(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cats_dic = [{'cats' : {encoder.categories_[0][i] : cat for i, cat in enumerate(cats)}} for cats in y_training]\n",
    "str_x_training = [str(x) for x in x_training] # spacy cannot handle numpy.str_ type which was used to create the list\n",
    "\n",
    "train_data = list(zip(str_x_training,list_cats_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed_spacy/training_data.json', 'w') as f:\n",
    "    json.dump(train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textcat': 14.87098198642434}\n",
      "{'textcat': 8.756967140657572}\n",
      "{'textcat': 5.2186675537482685}\n",
      "{'textcat': 3.1259694361547563}\n",
      "{'textcat': 2.489897636120153}\n",
      "{'textcat': 1.9491206841497595}\n",
      "{'textcat': 1.5800150970874858}\n",
      "{'textcat': 1.2287903863130951}\n",
      "{'textcat': 1.3678773922050786}\n",
      "{'textcat': 0.9998982481203083}\n"
     ]
    }
   ],
   "source": [
    "# Disabling other components\n",
    "n_iter=10\n",
    "\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "#    print(\"Training the model...\")\n",
    "#    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "\n",
    "    # Performing training\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "        print(losses)\n",
    "      # Calling the evaluate() function and printing the scores\n",
    "#        with textcat.model.use_params(optimizer.averages):\n",
    "#           scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "#        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  \n",
    "#              .format(losses['textcat'], scores['textcat_p'],\n",
    "#                      scores['textcat_r'], scores['textcat_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('../models/model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
