{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import json\n",
    "import random\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('training_set.json')\n",
    "data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training = [datapoint[\"intent\"] for datapoint in data]\n",
    "x_training = [datapoint[\"sentence\"] for datapoint in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase  :  Le meilleur cabriolet hybrid moins de 5m10 minimum 400 litres de coffre ?\n",
      "6035 6035\n"
     ]
    }
   ],
   "source": [
    "print(y_training[2], \" : \",x_training[2])\n",
    "print(len(y_training),len(x_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner', 'textcat']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "# Adding the built-in textcat component to the pipeline.\n",
    "textcat=nlp.create_pipe( \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"})\n",
    "nlp.add_pipe(textcat, last=True)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcat.add_label(\"find-train\")\n",
    "textcat.add_label(\"irrelevant\")\n",
    "textcat.add_label(\"find-flight\")\n",
    "textcat.add_label(\"find-restaurant\")\n",
    "textcat.add_label(\"purchase\")\n",
    "textcat.add_label(\"find-around-me\")\n",
    "textcat.add_label(\"provide-showtimes\")\n",
    "textcat.add_label(\"find-hotel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(train_texts,[{'cats': cats} for cats in train_cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling other components\n",
    "n_iter=10\n",
    "\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "\n",
    "    # Performing training\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "\n",
    "      # Calling the evaluate() function and printing the scores\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  \n",
    "              .format(losses['textcat'], scores['textcat_p'],\n",
    "                      scores['textcat_r'], scores['textcat_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Wiidi'\n",
      "/home/jovyan/Wiidi\n",
      "\u001b[1m\n",
      "=========================== Data format validation ===========================\u001b[0m\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/spacy/__main__.py\", line 33, in <module>\n",
      "    plac.call(commands[command], sys.argv[1:])\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/plac_core.py\", line 367, in call\n",
      "    cmd, result = parser.consume(arglist)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/plac_core.py\", line 232, in consume\n",
      "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/spacy/cli/debug_data.py\", line 85, in debug_data\n",
      "    corpus = GoldCorpus(train_path, dev_path)\n",
      "  File \"gold.pyx\", line 174, in spacy.gold.GoldCorpus.__init__\n",
      "  File \"gold.pyx\", line 185, in spacy.gold.GoldCorpus.write_msgpack\n",
      "  File \"gold.pyx\", line 230, in read_tuples\n",
      "  File \"gold.pyx\", line 503, in read_json_file\n",
      "  File \"gold.pyx\", line 465, in json_to_tuple\n",
      "KeyError: 'paragraphs'\n"
     ]
    }
   ],
   "source": [
    "%cd Wiidi\n",
    "!python -m spacy debug-data en ./training_set.json ./testing_set.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
